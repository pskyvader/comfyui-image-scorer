{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d71c55",
   "metadata": {},
   "source": [
    "# Polynomial Feature Expansion Experiment\n",
    "This notebook implements Polynomial Feature Expansion to capture non-linear relationships and improve R-squared scores, supplementing existing hyperparameter optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baa84a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root set to: c:\\ComfyUI\\trainer\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup Project Path and Imports\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Set project root\n",
    "p = Path.cwd().parent\n",
    "if str(p) not in sys.path:\n",
    "    sys.path.insert(0, str(p))\n",
    "print(\"Project root set to:\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f8fe5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load and Reload Configuration Modules\n",
    "import importlib\n",
    "import shared.config\n",
    "import training.run\n",
    "import training.data_utils\n",
    "\n",
    "# Reload to ensure fresh state\n",
    "importlib.reload(shared.config)\n",
    "from shared.config import config\n",
    "\n",
    "importlib.reload(training.run)\n",
    "importlib.reload(training.data_utils)\n",
    "\n",
    "from training.helpers import resolve_path\n",
    "from training.data_utils import get_filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82ee1882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from:\n",
      "Vectors: c:\\ComfyUI\\trainer\\prepare\\output\\vectors.jsonl\n",
      "Scores: c:\\ComfyUI\\trainer\\prepare\\output\\scores.jsonl\n",
      "Loading filtered data from cache: C:\\ComfyUI\\trainer\\training\\output\\filtered_data_cache.npz\n",
      "Data ready (cached). Filtered shape: (6105, 1308)\n",
      "Original Data Shape: X=(6105, 1308), y=(6105,)\n"
     ]
    }
   ],
   "source": [
    "# 3. Load and Preprocess Data\n",
    "# Load Configuration Paths\n",
    "vectors_path = resolve_path(config[\"vectors_file\"])\n",
    "scores_path = resolve_path(config[\"scores_file\"])\n",
    "\n",
    "print(f\"Loading data from:\\nVectors: {vectors_path}\\nScores: {scores_path}\")\n",
    "\n",
    "# Load and Filter Data\n",
    "# X is the feature matrix, y is the target scores\n",
    "X, y, kept_indices = get_filtered_data(vectors_path, scores_path)\n",
    "\n",
    "print(f\"Original Data Shape: X={X.shape}, y={y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "243c8b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Interaction Features to ADD to original data...\n",
      "Available RAM: 22.57 GB. Using Limit: 20.31 GB\n",
      "Input Features: 1308\n",
      "Total Potential Interactions: 854778\n",
      "Target Additive Features: 200\n",
      "Estimated Full Matrix Size (if instantiated): 38.94 GB\n",
      "Processing in batches of 156 samples...\n",
      "Pass 1: Computing Interaction Correlations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning Interactions: 100%|██████████| 6105/6105 [00:40<00:00, 152.28samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing scores...\n",
      "Selecting Top 200 interactions...\n",
      "Pass 2: Constructing Augmented Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Matrix: 100%|██████████| 6105/6105 [00:12<00:00, 474.88samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating with original features...\n",
      "Final Data Shape: (6105, 1508)\n",
      "Added 200 interaction features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Generate Additive Interaction Features (Batched)\n",
    "import psutil\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import f_regression\n",
    "import gc\n",
    "# Use standard tqdm which works well in VS Code text output if widgets aren't available\n",
    "from tqdm import tqdm \n",
    "\n",
    "print(\"Generating Interaction Features to ADD to original data...\")\n",
    "\n",
    "# Configuration\n",
    "# Strategy Change: Instead of filtering everything, we KEEP the original X (baseline)\n",
    "# and only append the top K *new* interaction features.\n",
    "# This ensures we don't perform worse than baseline, only better or equal.\n",
    "target_new_k = 200 # Add top 500 strongest interactions\n",
    "degrees = 2\n",
    "\n",
    "# Memory Check\n",
    "AVAILABLE_RAM = psutil.virtual_memory().available\n",
    "RAM_LIMIT = AVAILABLE_RAM * 0.90\n",
    "print(f\"Available RAM: {AVAILABLE_RAM/1024**3:.2f} GB. Using Limit: {RAM_LIMIT/1024**3:.2f} GB\")\n",
    "\n",
    "n_features_in = X.shape[1]\n",
    "n_samples = X.shape[0]\n",
    "\n",
    "# We use interaction_only=True because x^2 is often redundant with x after scaling,\n",
    "# but x*y captures unique relationships.\n",
    "poly = PolynomialFeatures(degree=degrees, include_bias=False, interaction_only=True)\n",
    "\n",
    "# Calculate expected total features (Original + Interactions)\n",
    "# n + n(n-1)/2\n",
    "n_features_total = n_features_in + (n_features_in * (n_features_in - 1)) // 2\n",
    "n_interactions = n_features_total - n_features_in\n",
    "\n",
    "print(f\"Input Features: {n_features_in}\")\n",
    "print(f\"Total Potential Interactions: {n_interactions}\")\n",
    "print(f\"Target Additive Features: {target_new_k}\")\n",
    "\n",
    "estimated_matrix_size = n_samples * n_features_total * 8 \n",
    "print(f\"Estimated Full Matrix Size (if instantiated): {estimated_matrix_size/1024**3:.2f} GB\")\n",
    "\n",
    "# Batch Size Calculation\n",
    "# We need to compute stats for the interaction columns. \n",
    "# We'll generate the full poly expansion per batch, but only accumulate stats for the interaction part.\n",
    "BATCH_MEMORY_TARGET = 1 * 1024**3 \n",
    "bytes_per_row = n_features_total * 8\n",
    "batch_size = int(BATCH_MEMORY_TARGET / bytes_per_row)\n",
    "if batch_size < 1: batch_size = 1\n",
    "if batch_size > 5000: batch_size = 5000 \n",
    "\n",
    "print(f\"Processing in batches of {batch_size} samples...\")\n",
    "\n",
    "# Accumulators for correlation calculation (Only for interactions)\n",
    "# We only care about columns [n_features_in : ]\n",
    "sum_x = np.zeros(n_interactions)\n",
    "sum_x_sq = np.zeros(n_interactions)\n",
    "sum_xy = np.zeros(n_interactions)\n",
    "sum_y = 0\n",
    "sum_y_sq = 0\n",
    "N = 0\n",
    "\n",
    "# Check if single row is too big (rare with interaction_only, but possible)\n",
    "if bytes_per_row > RAM_LIMIT * 0.8:\n",
    "    print(\"Warning: Matrix row size is extremely large. This might be slow or OOM.\")\n",
    "\n",
    "# Pass 1: Statistics on Interactions\n",
    "print(\"Pass 1: Computing Interaction Correlations...\")\n",
    "\n",
    "with tqdm(total=n_samples, desc=\"Scanning Interactions\", unit=\"samples\") as pbar:\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        end_idx = min(i + batch_size, n_samples)\n",
    "        \n",
    "        X_batch = X[i:end_idx]\n",
    "        y_batch = y[i:end_idx]\n",
    "        \n",
    "        # This returns [X_batch, Interactions]\n",
    "        # We only want the interaction part\n",
    "        X_poly_full = poly.fit_transform(X_batch)\n",
    "        X_inter_batch = X_poly_full[:, n_features_in:]\n",
    "        \n",
    "        # Accumulate\n",
    "        sum_x += np.sum(X_inter_batch, axis=0)\n",
    "        sum_x_sq += np.sum(X_inter_batch ** 2, axis=0)\n",
    "        sum_xy += np.dot(X_inter_batch.T, y_batch)\n",
    "        \n",
    "        # Only do Y stats once (redundant but cheap)\n",
    "        if N == 0: \n",
    "            # Actually we need sum over all batches for Y, doing incrementally is fine\n",
    "            pass\n",
    "        sum_y += np.sum(y_batch)\n",
    "        sum_y_sq += np.sum(y_batch ** 2)\n",
    "        \n",
    "        N += (end_idx - i)\n",
    "        \n",
    "        del X_poly_full, X_inter_batch\n",
    "        gc.collect()\n",
    "        pbar.update(end_idx - i)\n",
    "\n",
    "# Compute Correlations\n",
    "print(\"Computing scores...\")\n",
    "numerator = (N * sum_xy) - (sum_x * sum_y)\n",
    "denominator_x = (N * sum_x_sq) - (sum_x ** 2)\n",
    "denominator_y = (N * sum_y_sq) - (sum_y ** 2)\n",
    "\n",
    "denominator_x[denominator_x <= 0] = 1e-10\n",
    "denominator = np.sqrt(denominator_x * denominator_y)\n",
    "correlation = numerator / denominator\n",
    "f_scores = (correlation ** 2) / (1 - correlation ** 2 + 1e-10) * (N - 2)\n",
    "f_scores = np.nan_to_num(f_scores, nan=0.0)\n",
    "\n",
    "# Select Top K Interactions\n",
    "k = min(target_new_k, n_interactions)\n",
    "print(f\"Selecting Top {k} interactions...\")\n",
    "top_k_indices_local = np.argsort(f_scores)[-k:]\n",
    "top_k_indices_local = np.sort(top_k_indices_local) # Indices relative to the interaction-only block\n",
    "\n",
    "# Pass 2: Construction\n",
    "print(\"Pass 2: Constructing Augmented Dataset...\")\n",
    "# Result will be Original X + Selected Interactions\n",
    "X_interactions = np.zeros((n_samples, k), dtype=X.dtype)\n",
    "\n",
    "with tqdm(total=n_samples, desc=\"Building Matrix\", unit=\"samples\") as pbar:\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        end_idx = min(i + batch_size, n_samples)\n",
    "        X_batch = X[i:end_idx]\n",
    "        \n",
    "        X_poly_full = poly.fit_transform(X_batch)\n",
    "        # Select from the interaction part\n",
    "        # Shift indices? No, top_k_indices_local is 0-based relative to the interaction slice\n",
    "        X_interactions[i:end_idx] = X_poly_full[:, n_features_in:][:, top_k_indices_local]\n",
    "        \n",
    "        del X_poly_full\n",
    "        gc.collect()\n",
    "        pbar.update(end_idx - i)\n",
    "\n",
    "print(\"Concatenating with original features...\")\n",
    "X_selected = np.hstack([X, X_interactions])\n",
    "\n",
    "print(f\"Final Data Shape: {X_selected.shape}\")\n",
    "print(f\"Added {X_selected.shape[1] - X.shape[1]} interaction features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd6915b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection completed in previous step due to memory optimizations.\n",
      "X_selected shape: (6105, 1508)\n"
     ]
    }
   ],
   "source": [
    "# 5. Review Selection (Already done in batched step)\n",
    "print(\"Feature selection completed in previous step due to memory optimizations.\")\n",
    "print(f\"X_selected shape: {X_selected.shape}\")\n",
    "# We skip re-running SelectKBest because we manually computed it to save RAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "909a6a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Models using 5-Fold Cross-Validation...\n",
      "Loaded Top LightGBM Config:\n",
      "{'learning_rate': 0.44550000000000006, 'n_estimators': 675, 'num_leaves': 372, 'max_depth': 1, 'min_child_samples': 90, 'reg_alpha': 8.1, 'reg_lambda': 4.099784786849741, 'subsample': 0.11979000000000002, 'colsample_bytree': 0.1, 'min_split_gain': 0.5, 'device': 'gpu', 'verbosity': -1}\n",
      "\n",
      "Evaluating Baseline (LightGBM on Original X)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ComfyUI\\trainer\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\ComfyUI\\trainer\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\ComfyUI\\trainer\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\ComfyUI\\trainer\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\ComfyUI\\trainer\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Enhanced (LightGBM on Original + Interactions)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ComfyUI\\trainer\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\ComfyUI\\trainer\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\ComfyUI\\trainer\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\ComfyUI\\trainer\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline R2: 0.384593 (+/- 0.014043)\n",
      "Polynomial R2: 0.391540 (+/- 0.009532)\n",
      "Improvement: 0.006946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ComfyUI\\trainer\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 6. Evaluate Base vs Polynomial Models\n",
    "print(\"Evaluating Models using 5-Fold Cross-Validation...\")\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Load Top Config for LightGBM\n",
    "try:\n",
    "    training_cfg = config[\"training\"]\n",
    "    # Access 'top' directly if it exists\n",
    "    if \"top\" in training_cfg:\n",
    "        top_config = training_cfg[\"top\"]\n",
    "    else:\n",
    "        top_config = None\n",
    "except KeyError:\n",
    "    top_config = None\n",
    "\n",
    "if top_config:\n",
    "    print(\"Loaded Top LightGBM Config:\")\n",
    "    # Filter out metadata keys AND early_stopping_rounds\n",
    "    ignore_keys = ['best_score', 'training_time', 'early_stopping_rounds']\n",
    "    clean_params = {k: v for k, v in top_config.items() if k not in ignore_keys}\n",
    "    \n",
    "    # Handle device\n",
    "    device_setting = \"cpu\"\n",
    "    if \"device\" in training_cfg:\n",
    "        device_setting = training_cfg[\"device\"]\n",
    "        if device_setting == \"cuda\":\n",
    "            device_setting = \"gpu\"\n",
    "            \n",
    "    clean_params['device'] = device_setting\n",
    "    clean_params['verbosity'] = -1\n",
    "    print(clean_params)\n",
    "else:\n",
    "    print(\"Warning: No 'top' config found. Using defaults.\")\n",
    "    clean_params = {\"n_estimators\": 500, \"learning_rate\": 0.05, \"verbosity\": -1}\n",
    "\n",
    "print(\"\\nEvaluating Baseline (LightGBM on Original X)...\")\n",
    "# Note: LightGBM doesn't usually need StandardScaler, but it doesn't hurt. \n",
    "# We'll pass raw data to LGBM to be idiomatic.\n",
    "model_base = lgb.LGBMRegressor(**clean_params)\n",
    "scores_base = cross_val_score(model_base, X, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(\"\\nEvaluating Enhanced (LightGBM on Original + Interactions)...\")\n",
    "# We use the same params. Ideally we might re-tune for more features, but this is a direct comparison.\n",
    "model_poly = lgb.LGBMRegressor(**clean_params)\n",
    "scores_poly = cross_val_score(model_poly, X_selected, y, cv=kf, scoring='r2')\n",
    "\n",
    "print(f\"\\nBaseline R2: {np.mean(scores_base):.6f} (+/- {np.std(scores_base):.6f})\")\n",
    "print(f\"Polynomial R2: {np.mean(scores_poly):.6f} (+/- {np.std(scores_poly):.6f})\")\n",
    "\n",
    "improvement = np.mean(scores_poly) - np.mean(scores_base)\n",
    "print(f\"Improvement: {improvement:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80a4951c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement found! Saving transformed dataset...\n",
      "Saved transformed feature set to: c:\\ComfyUI\\trainer\\training\\output\\poly_selected_data.npz\n",
      "You can modify 'training/data_utils.py' or 'training/run.py' to load this file if it exists.\n"
     ]
    }
   ],
   "source": [
    "# 7. Save Enhanced Feature Configuration\n",
    "if improvement > 0.001:\n",
    "    print(\"Improvement found! Saving transformed dataset...\")\n",
    "    \n",
    "    # Save the transformed dataset for the Training Loop to pick up\n",
    "    out_dir = resolve_path(\"training/output\")\n",
    "    import os\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    output_path = resolve_path(\"training/output/poly_selected_data.npz\")\n",
    "    \n",
    "    np.savez(output_path, X=X_selected, y=y)\n",
    "    print(f\"Saved transformed feature set to: {output_path}\")\n",
    "    print(\"You can modify 'training/data_utils.py' or 'training/run.py' to load this file if it exists.\")\n",
    "else:\n",
    "    print(\"No significant improvement. Polynomial features might not be necessary or need tuning, or LightGBM handles interactions well enough already.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
